「文脈木重み付け法と増分文脈アルゴリズム」というテーマで話します。

文脈木重み付け法は、情報源と指摘情報源を仮定し、それを推定することでデータ圧縮を実現しようという方法です。

まずは、文脈木重み付け法の前に、算術符号について述べます。

バイナリの時、つまりシンボルが0と1の時の算術符号化の様子を表したのがこの図です。算術符号は0と1の間の実数値をとり、例えば入力系列の長さが2の時、符号の値がこの辺りなら入力は00、この辺りなら入力は10であるということになります。

符号化の際には累積確率、例えば0に関してはここで、10に関してはここになりますが、これを逐次的に求めていくことができます。01の場合は、まず入力系列が0の時の累積確率を求めて、それを使って、それにp(00)を加えるという形で、01の累積確率を求めます。この式のこの部分でp(00)を求めていることになります。

こうして得られた符号の符号長は、このように、その系列に与えられる確率の負の対数に2を加えたものになります。つまりこの確率が大きければ、その分符号長は短くなります。

文脈木重み付け法は、この予測確率を与えます。

まずはポストフィックスについて。例えばこういう系列があった時、これのポストフィックスは、空の文字列λ、1、01、１０１、などです。ポストフィックス木とは、その木のノードのポストフィックスが全てその木のノードであるようなものをいいます。例えば、この木なら、00のポストフィックスである0とλがこの木のノードになっている、という感じです。文脈木は、このポストフィックス木の形をしています。文脈木は、各ノードが、θ s aというパラメータを持っています。これは、文脈sの後に文字aが来る確率を表します。

ポストフィックス木Tを考えた時に、その外部葉集合∂Tをこのように定義します。Tの葉の全ての子の集合です。

例として、この木情報源が101という系列を生じる確率がどう表せるかを示します。文脈を決定するために、x0以前の系列を00と仮定しています。すると101を生じる確率は、00の後に1、1の後に0、10の後に1が出ているので、このようになって、これはこのように表されます。n s aは、sの後に実際にaが出たカウントです。

しかし、このθは未知なので、何らかの形で推定する必要があります。これを推定するために、KT推定量を用います。これは、n s 0とn s 1、つまり実際に、文脈sの後に0あるいは1が表れたカウントから、この値、θを含んだこの値を推定しようというものです。この推定量には、このように逐次的に計算できるという特性があります。

ここまでは、情報源の木の形Tが分かっているものとして考えてきました。しかし、実際には、木の形は分かりません。そこで、Tに、このような事前分布を与えます。γは0から1の値を取り、あるノードに関して、γとγバーを加えると1でなければなりません。通常は、γとして1/2を与えます。

そしてこのTの事前分布と、Tを仮定した時の外部葉におけるKT推定量の積との混合を取ることで、文脈木重み付け法の与える推定確率が得られます。この確率を使って算術符号化します。これは、このように、木に関して再帰的な計算で求めることができます。このT´は、これだと無限の深さの木を扱わなければならないので、有限で打ち切るために、考える最大の木としてT´を与えています。

先ほど例としてあげた系列を入力系列に、このようなT´、λと1を含むようなT'に関して文脈木重み付け法を適用すると、このようになります。こうして得られるルートノードの重み付け確率pwが入力系列に対する推定確率となります。

文脈木の更新の手間は、外部葉の深さに比例程度になります。

ここまでは、文脈を得るために過去の系列、x0以前の系列を仮定していましたが、増分文脈アルゴリズムでは、過去の系列を仮定せずに処理することができます。

このアルゴリズムでは、Tが空の状態から始まります。そして、外部葉の中から、文脈となっているものを一つずつTに加えていきます。

まず、x1=1のとき、この文脈はλなので、λをTに加えます。x2=0のとき、この文脈は1なので、1をTに加えます。x3=1の時、この文脈は0なので、0をTに加えます。

KT推定量peは前述のものと同じなので問題ないのですが、重み付け確率pwの計算は同じ様には行きません。時刻tが2の時を見ると、ルートノードのカウントの和は2ですが、そのこのカウントの和は1となっています。足りないカウント1はどこに行ったのかというと、本来なら、このλを追加した時に、このノードの子のどちらかのカウントが1になっているはずです。しかし、過去の文脈が使えないので、どちらにカウント1が行くべきなのか分かりません。そこで、実際に存在する子とは別に、もう一つ子ノードがあって、この子ノードがカウント1を持っていると考えます。カウントが1の時のKT推定量は、バイナリなら1/2なので、再帰式で、子ノードの重み付け確率の積を取る時に、もう一つの子ノードの分1/2をかけます。これが、増分文脈アルゴリズムにおいて、重み付け確率を求める式になります。vtはTに追加されたノードで、追加されたノードから、ボトムアップに計算されます。
